(base) prakritipaul@Prakritis-MacBook-Pro Demo % python -i Demo.py
2024-01-17 23:26:19.124102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/Users/prakritipaul/Git/GENELink/Code/utils.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)  i = torch.LongTensor([coo.row, coo.col])
Epoch:1 train loss:131.83843991160393 AUC:0.867 AUPR:0.729
Epoch:2 train loss:99.75595420598984 AUC:0.874 AUPR:0.729
Epoch:3 train loss:98.77929177880287 AUC:0.879 AUPR:0.763
Epoch:4 train loss:98.41566604375839 AUC:0.876 AUPR:0.753
Epoch:5 train loss:98.5259690284729 AUC:0.878 AUPR:0.759
Epoch:6 train loss:98.86151206493378 AUC:0.878 AUPR:0.761
Epoch:7 train loss:98.57371878623962 AUC:0.879 AUPR:0.758
Epoch:8 train loss:98.80462589859962 AUC:0.875 AUPR:0.748
Epoch:9 train loss:98.53731352090836 AUC:0.877 AUPR:0.753
Epoch:10 train loss:98.44885170459747 AUC:0.879 AUPR:0.763
Epoch:11 train loss:98.4096550643444 AUC:0.877 AUPR:0.755
Epoch:12 train loss:98.58182227611542 AUC:0.876 AUPR:0.747
Epoch:13 train loss:98.85533168911934 AUC:0.872 AUPR:0.707
Epoch:14 train loss:98.42536038160324 AUC:0.878 AUPR:0.748
Epoch:15 train loss:98.63365119695663 AUC:0.878 AUPR:0.752
Epoch:16 train loss:98.50715172290802 AUC:0.874 AUPR:0.716
Epoch:17 train loss:98.6394456923008 AUC:0.877 AUPR:0.754
Epoch:18 train loss:98.53241619467735 AUC:0.875 AUPR:0.736
Epoch:19 train loss:98.60417485237122 AUC:0.875 AUPR:0.747
Epoch:20 train loss:98.61718788743019 AUC:0.879 AUPR:0.758
Epoch:21 train loss:98.61794129014015 AUC:0.874 AUPR:0.703
Epoch:22 train loss:98.68901690840721 AUC:0.874 AUPR:0.713
Epoch:23 train loss:98.48303064703941 AUC:0.877 AUPR:0.734
Epoch:24 train loss:98.56402283906937 AUC:0.878 AUPR:0.745
Epoch:25 train loss:98.8038389980793 AUC:0.875 AUPR:0.750
Epoch:26 train loss:98.5786023736 AUC:0.875 AUPR:0.745
Epoch:27 train loss:98.67193484306335 AUC:0.878 AUPR:0.761
Epoch:28 train loss:98.70947453379631 AUC:0.878 AUPR:0.739
Epoch:29 train loss:98.71121698617935 AUC:0.876 AUPR:0.753
Epoch:30 train loss:98.6494409441948 AUC:0.878 AUPR:0.749
Epoch:31 train loss:98.75879433751106 AUC:0.879 AUPR:0.759
Epoch:32 train loss:98.3906626701355 AUC:0.879 AUPR:0.758
Epoch:33 train loss:98.612908244133 AUC:0.874 AUPR:0.730
Epoch:34 train loss:98.6810474395752 AUC:0.878 AUPR:0.755
Epoch:35 train loss:98.5425890982151 AUC:0.878 AUPR:0.743
Epoch:36 train loss:98.53574061393738 AUC:0.879 AUPR:0.756
Epoch:37 train loss:98.62743681669235 AUC:0.876 AUPR:0.740
Epoch:38 train loss:98.64743912220001 AUC:0.878 AUPR:0.756
Epoch:39 train loss:98.66928842663765 AUC:0.874 AUPR:0.740
Epoch:40 train loss:98.5576459467411 AUC:0.874 AUPR:0.718
Epoch:41 train loss:98.65980476140976 AUC:0.879 AUPR:0.754
Epoch:42 train loss:98.88404539227486 AUC:0.876 AUPR:0.749
Epoch:43 train loss:98.57901751995087 AUC:0.876 AUPR:0.741
Epoch:44 train loss:98.53677979111671 AUC:0.879 AUPR:0.764
Epoch:45 train loss:98.44782549142838 AUC:0.880 AUPR:0.766
Epoch:46 train loss:98.69510847330093 AUC:0.873 AUPR:0.718
Epoch:47 train loss:98.6985630095005 AUC:0.877 AUPR:0.752
Epoch:48 train loss:98.75664174556732 AUC:0.877 AUPR:0.754
Epoch:49 train loss:98.75548148155212 AUC:0.874 AUPR:0.738
Epoch:50 train loss:98.67331790924072 AUC:0.876 AUPR:0.751
Epoch:51 train loss:98.69979646801949 AUC:0.879 AUPR:0.759
Epoch:52 train loss:98.93429440259933 AUC:0.879 AUPR:0.765
Epoch:53 train loss:98.46168634295464 AUC:0.874 AUPR:0.736
Epoch:54 train loss:98.38017937541008 AUC:0.879 AUPR:0.754
Epoch:55 train loss:98.53229027986526 AUC:0.878 AUPR:0.749
Epoch:56 train loss:98.86082211136818 AUC:0.880 AUPR:0.763
Epoch:57 train loss:98.6251731812954 AUC:0.877 AUPR:0.737
Epoch:58 train loss:98.61631992459297 AUC:0.878 AUPR:0.755
Epoch:59 train loss:98.76264637708664 AUC:0.876 AUPR:0.746
Epoch:60 train loss:98.76928558945656 AUC:0.877 AUPR:0.732
Epoch:61 train loss:98.61675357818604 AUC:0.879 AUPR:0.760
Epoch:62 train loss:98.57698160409927 AUC:0.876 AUPR:0.746
Epoch:63 train loss:98.83769124746323 AUC:0.879 AUPR:0.757
Epoch:64 train loss:98.67472141981125 AUC:0.876 AUPR:0.748
Epoch:65 train loss:98.7121550142765 AUC:0.879 AUPR:0.762
Epoch:66 train loss:98.50935873389244 AUC:0.877 AUPR:0.750
Epoch:67 train loss:98.72201877832413 AUC:0.876 AUPR:0.736
Epoch:68 train loss:98.44669085741043 AUC:0.878 AUPR:0.752
Epoch:69 train loss:98.43544682860374 AUC:0.878 AUPR:0.739
Epoch:70 train loss:98.84629127383232 AUC:0.880 AUPR:0.754
Epoch:71 train loss:98.80921438336372 AUC:0.877 AUPR:0.748
Epoch:72 train loss:98.8277095258236 AUC:0.878 AUPR:0.756
Epoch:73 train loss:98.69389787316322 AUC:0.880 AUPR:0.761
Epoch:74 train loss:98.61717650294304 AUC:0.878 AUPR:0.753
Epoch:75 train loss:98.98344641923904 AUC:0.877 AUPR:0.753
Epoch:76 train loss:98.51612773537636 AUC:0.878 AUPR:0.758
Epoch:77 train loss:98.78067383170128 AUC:0.874 AUPR:0.723
Epoch:78 train loss:98.63879492878914 AUC:0.877 AUPR:0.739
Epoch:79 train loss:98.48092767596245 AUC:0.879 AUPR:0.754
Epoch:80 train loss:98.48922017216682 AUC:0.879 AUPR:0.752
Epoch:81 train loss:98.80844742059708 AUC:0.878 AUPR:0.757
Epoch:82 train loss:98.44367006421089 AUC:0.878 AUPR:0.753
Epoch:83 train loss:98.63828280568123 AUC:0.879 AUPR:0.758
Epoch:84 train loss:98.41488406062126 AUC:0.878 AUPR:0.756
Epoch:85 train loss:98.67178246378899 AUC:0.874 AUPR:0.717
Epoch:86 train loss:98.70548391342163 AUC:0.879 AUPR:0.756
Epoch:87 train loss:98.9075910449028 AUC:0.880 AUPR:0.750
Epoch:88 train loss:98.79298758506775 AUC:0.879 AUPR:0.761
Epoch:89 train loss:98.47770535945892 AUC:0.875 AUPR:0.741
Epoch:90 train loss:98.8135838508606 AUC:0.878 AUPR:0.747
AUC:0.8704808091446765 AUPRC:0.7299813341469691
>>> import pickle
>>> my_pickle = "/Users/prakritipaul/Git/GENELink/Demo/model/mESC 500.pkl"
>>> with open(my_pickle, "rb") as f:
...     data = pickle.load(f)
... 
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
_pickle.UnpicklingError: A load persistent id instruction was encountered,
but no persistent_load function was specified.
>>> model
GENELink(
  (ConvLayer1_AttentionHead0): AttentionLayer()
  (ConvLayer1_AttentionHead1): AttentionLayer()
  (ConvLayer1_AttentionHead2): AttentionLayer()
  (ConvLayer2_AttentionHead0): AttentionLayer()
  (ConvLayer2_AttentionHead1): AttentionLayer()
  (ConvLayer2_AttentionHead2): AttentionLayer()
  (tf_linear1): Linear(in_features=64, out_features=32, bias=True)
  (target_linear1): Linear(in_features=64, out_features=32, bias=True)
  (tf_linear2): Linear(in_features=32, out_features=16, bias=True)
  (target_linear2): Linear(in_features=32, out_features=16, bias=True)
)
>>> test = model.state_dict()
>>> test
OrderedDict([('ConvLayer1_AttentionHead0.weight', tensor([[ 0.0128, -0.0500,  0.0178,  ..., -0.1159, -0.0940,  0.0388],
        [-0.0937, -0.1206,  0.0731,  ..., -0.0878,  0.0024,  0.1305],
        [ 0.1148,  0.0015, -0.1163,  ...,  0.1287,  0.1282,  0.0590],
        ...,
        [-0.0609, -0.0705, -0.0611,  ..., -0.0499,  0.0428,  0.0372],
        [ 0.0422, -0.1276, -0.0326,  ..., -0.0095, -0.0475,  0.0891],
        [-0.0682,  0.0641, -0.1236,  ...,  0.1041, -0.0483,  0.0866]])), ('ConvLayer1_AttentionHead0.weight_interact', tensor([[ 0.1300, -0.0797, -0.0560,  ..., -0.1265,  0.0975,  0.0928],
        [-0.0230, -0.1273, -0.1434,  ..., -0.0776,  0.0848, -0.1460],
        [ 0.0124, -0.0664, -0.0110,  ..., -0.0970, -0.1031, -0.1264],
        ...,
        [ 0.1192,  0.0175, -0.1177,  ..., -0.0954,  0.0789, -0.0020],
        [ 0.1453, -0.0087,  0.0613,  ...,  0.1402,  0.0748, -0.1096],
        [ 0.0723,  0.0196,  0.0817,  ...,  0.1175,  0.1272, -0.1088]])), ('ConvLayer1_AttentionHead0.a', tensor([[ 7.6976e-03],
        [-1.6689e-01],
        [-7.6132e-02],
        [-8.7526e-02],
        [-1.5079e-01],
        [ 5.3552e-02],
        [ 1.8571e-01],
        [ 1.0442e-01],
        [-2.2677e-01],
        [-2.1557e-01],
        [-2.0674e-01],
        [-3.7609e-02],
        [-5.8117e-03],
        [-5.1364e-02],
        [-9.6646e-02],
        [-7.9187e-02],
        [ 3.1695e-02],
        [-1.1555e-02],
        [ 1.1881e-01],
        [ 1.4124e-01],
        [-6.7754e-03],
        [ 2.9669e-02],
        [ 6.8742e-02],
        [-2.1043e-01],
        [-1.9043e-01],
        [-1.2452e-01],
        [-1.2866e-01],
        [-2.9144e-02],
        [-7.1943e-02],
        [-4.2849e-02],
        [ 4.2328e-02],
        [-1.5903e-01],
        [-2.0168e-01],
        [-6.4342e-02],
        [-4.7020e-02],
        [-1.3999e-01],
        [-9.5419e-02],
        [ 7.1269e-02],
        [-2.0142e-01],
        [-1.6646e-01],
        [-1.2218e-01],
        [-1.2459e-01],
        [-1.6366e-01],
        [-7.4983e-03],
        [ 1.5001e-01],
        [-5.6577e-02],
        [ 6.9645e-02],
        [-1.2252e-01],
        [-1.2845e-01],
        [ 1.4082e-01],
        [ 2.1271e-01],
        [ 8.3382e-02],
        [ 1.9729e-01],
        [ 7.6684e-02],
        [ 2.9014e-02],
        [ 2.1188e-01],
        [ 1.6748e-01],
        [ 7.5692e-02],
        [-1.2517e-01],
        [-4.8273e-02],
        [ 1.7511e-01],
        [-1.0809e-01],
        [-1.9434e-01],
        [-8.3494e-02],
        [ 1.2650e-01],
        [ 6.1748e-02],
        [ 6.9341e-02],
        [-1.3716e-02],
        [ 1.0916e-01],
        [ 8.6530e-04],
        [ 8.1837e-03],
        [ 1.0274e-01],
        [ 1.4519e-01],
        [ 1.4804e-02],
        [ 2.2055e-01],
        [ 1.8679e-01],
        [ 1.1106e-01],
        [-2.1016e-01],
        [-3.9158e-02],
        [-2.2583e-02],
        [ 1.3122e-01],
        [-1.6766e-01],
        [ 2.1494e-01],
        [-2.0206e-02],
        [-1.0227e-01],
        [ 1.7859e-01],
        [ 1.2815e-01],
        [ 1.5135e-01],
        [-1.0790e-01],
        [-6.0624e-02],
        [-1.3312e-01],
        [ 1.7075e-01],
        [ 1.9839e-01],
        [-1.0533e-01],
        [ 2.5021e-02],
        [ 8.2563e-02],
        [ 3.2987e-02],
        [ 7.5608e-04],
        [ 4.9629e-02],
        [-1.9822e-01],
        [ 1.6035e-01],
        [ 1.6581e-01],
        [ 1.3842e-01],
        [ 1.9383e-01],
        [-4.1352e-02],
        [-9.4597e-02],
        [-2.3267e-01],
        [ 5.1275e-02],
        [-2.5689e-03],
        [ 1.5289e-01],
        [ 1.7192e-01],
        [ 6.1332e-02],
        [ 2.1250e-01],
        [-1.1225e-04],
        [ 2.6213e-02],
        [ 2.7031e-02],
        [ 4.4030e-02],
        [ 9.3469e-02],
        [ 1.6659e-01],
        [-6.5130e-02],
        [-1.2220e-01],
        [-1.5326e-01],
        [-1.2399e-01],
        [ 2.7683e-02],
        [-1.9226e-01],
        [-5.7137e-02],
        [ 1.5025e-01],
        [-1.8248e-01],
        [-1.8785e-01],
        [-1.2818e-02],
        [-3.3466e-03],
        [-1.8844e-01],
        [ 1.9642e-01],
        [ 3.0914e-02],
        [ 6.6600e-02],
        [ 2.3638e-01],
        [ 1.3444e-01],
        [ 6.3835e-04],
        [ 1.6368e-01],
        [ 6.1779e-02],
        [ 1.2278e-01],
        [-6.6442e-02],
        [-4.6126e-02],
        [-7.5891e-03],
        [ 2.9446e-02],
        [-6.1383e-02],
        [-7.8675e-02],
        [-1.5078e-01],
        [ 1.2082e-01],
        [ 3.8022e-02],
        [ 6.0197e-02],
        [-9.3112e-02],
        [ 1.6495e-01],
        [ 2.1655e-01],
        [ 8.7565e-02],
        [ 1.3413e-01],
        [-8.0232e-02],
        [ 4.2962e-02],
        [ 1.0770e-01],
        [-2.2389e-01],
        [ 1.3907e-01],
        [-1.6020e-01],
        [-3.8076e-02],
        [-5.4738e-02],
        [-3.1020e-02],
        [ 4.5406e-03],
        [-1.0246e-01],
        [-1.5343e-01],
        [-3.5461e-03],
        [-5.9318e-02],
        [-4.4647e-02],
        [-1.0012e-01],
        [ 1.4561e-01],
        [-1.1739e-01],
        [-1.5715e-02],
        [ 4.0049e-02],
        [ 1.3263e-01],
        [-1.4593e-01],
        [ 1.9962e-02],
        [ 1.0000e-01],
        [ 1.3396e-02],
        [-2.3152e-01],
        [ 7.3125e-02],
        [-8.0400e-02],
        [-5.6539e-02],
        [-4.3136e-02],
        [ 1.8555e-01],
        [-1.7704e-03],
        [ 9.8636e-02],
        [ 1.4769e-01],
        [-1.4387e-02],
        [-1.5707e-01],
        [-1.2239e-01],
        [-8.7931e-02],
        [ 1.7831e-01],
        [-1.3091e-01],
        [ 2.1791e-01],
        [-1.9598e-03],
        [-1.9105e-01],
        [-5.8220e-02],
        [-1.2222e-01],
        [ 1.1779e-01],
        [-1.6352e-01],
        [ 5.3610e-02],
        [-4.6134e-02],
        [ 2.1925e-01],
        [ 1.4000e-01],
        [-2.1046e-02],
        [ 5.7692e-02],
        [ 2.2860e-01],
        [ 5.1982e-02],
        [ 1.9706e-01],
        [-1.2747e-01],
        [-7.6875e-02],
        [-8.8351e-02],
        [ 1.4079e-01],
        [ 1.7248e-01],
        [-1.8414e-01],
        [ 1.1349e-01],
        [-2.7255e-02],
        [-1.3585e-01],
        [-1.6244e-01],
        [-1.7854e-01],
        [-1.0718e-01],
        [ 1.3043e-01],
        [-2.2125e-02],
        [-1.5677e-01],
        [-1.7502e-01],
        [ 1.7222e-01],
        [-3.1831e-03],
        [ 4.4904e-02],
        [ 1.3964e-01],
        [-5.2529e-02],
        [ 2.4097e-01],
        [ 7.5348e-02],
        [ 4.8590e-02],
        [-2.2110e-02],
        [ 2.4488e-02],
        [ 8.0122e-03],
        [ 9.2391e-02],
        [-8.5425e-02],
        [-1.3002e-01],
        [ 2.0831e-01],
        [-1.6283e-01],
        [-1.6677e-01],
        [ 1.0878e-01],
        [ 2.5493e-02],
        [ 1.8431e-01],
        [-3.5945e-02],
        [-9.9741e-02],
        [-1.9590e-01],
        [-1.0511e-01],
        [ 1.0424e-01],
        [ 8.3269e-02],
        [ 6.7779e-02],
        [ 1.2447e-01]])), ('ConvLayer1_AttentionHead0.bias', tensor([ 0.0207,  0.0161, -0.0321,  0.0164, -0.0297,  0.0154,  0.0295,  0.0082,
        -0.0198, -0.0200,  0.0017,  0.0167,  0.0244, -0.0147,  0.0097, -0.0047,
        -0.0219,  0.0222,  0.0159,  0.0346, -0.0271,  0.0173,  0.0090,  0.0093,
         0.0019, -0.0002,  0.0114,  0.0068, -0.0255, -0.0051, -0.0218,  0.0198,
        -0.0052,  0.0020, -0.0013,  0.0156,  0.0122, -0.0074,  0.0037,  0.0120,
        -0.0210, -0.0023,  0.0220,  0.0068, -0.0037,  0.0014,  0.0074, -0.0043,
         0.0245,  0.0012,  0.0297, -0.0248, -0.0218,  0.0190, -0.0301,  0.0284,
         0.0228, -0.0093, -0.0071,  0.0169, -0.0272, -0.0030, -0.0324, -0.0049,
        -0.0007, -0.0144, -0.0338,  0.0172,  0.0210, -0.0228, -0.0169, -0.0157,
         0.0003,  0.0218, -0.0150,  0.0133,  0.0204,  0.0021, -0.0057,  0.0140,
        -0.0239,  0.0035,  0.0170, -0.0021, -0.0097,  0.0132, -0.0160,  0.0019,
         0.0048,  0.0179, -0.0174,  0.0119,  0.0078,  0.0017,  0.0057,  0.0155,
        -0.0137, -0.0058, -0.0166,  0.0036,  0.0025, -0.0070,  0.0104, -0.0155,
         0.0194, -0.0107, -0.0002, -0.0195,  0.0157, -0.0010, -0.0012, -0.0258,
         0.0012, -0.0189, -0.0039, -0.0097,  0.0056, -0.0053, -0.0124,  0.0178,
        -0.0203, -0.0126,  0.0126, -0.0123, -0.0297, -0.0139, -0.0281, -0.0291])), ('ConvLayer1_AttentionHead1.weight', tensor([[-0.0642, -0.0907, -0.0797,  ...,  0.0393, -0.0528,  0.0427],
        [-0.0710,  0.0505,  0.0868,  ...,  0.0168,  0.1164,  0.0581],
        [ 0.0680, -0.0190, -0.1378,  ...,  0.1183,  0.0932,  0.1281],
        ...,
        [ 0.0434, -0.0960, -0.0098,  ..., -0.1350,  0.0139, -0.0430],
        [-0.0574, -0.1028,  0.0439,  ..., -0.0363,  0.0680, -0.0239],
        [-0.0511, -0.0086, -0.1145,  ...,  0.0466, -0.0064, -0.1265]])), ('ConvLayer1_AttentionHead1.weight_interact', tensor([[-0.0513, -0.1276,  0.1264,  ..., -0.0864,  0.0551, -0.0353],
        [ 0.0004, -0.0806, -0.0547,  ...,  0.0333, -0.0267,  0.1341],
        [-0.0756,  0.1292, -0.0110,  ...,  0.0130,  0.0463,  0.1474],
        ...,
        [-0.1055,  0.1314, -0.0777,  ...,  0.0194, -0.0351, -0.0263],
        [ 0.0333, -0.0656, -0.0243,  ...,  0.0938,  0.0338,  0.0978],
        [-0.0316, -0.1357, -0.0088,  ...,  0.0694,  0.1202,  0.0904]])), ('ConvLayer1_AttentionHead1.a', tensor([[ 0.0452],
        [-0.0506],
        [-0.1014],
        [-0.1599],
        [-0.1635],
        [-0.0032],
        [-0.1308],
        [-0.1857],
        [ 0.0992],
        [ 0.0881],
        [-0.1898],
        [-0.2141],
        [ 0.1292],
        [-0.1076],
        [ 0.0180],
        [ 0.0433],
        [ 0.1382],
        [ 0.0678],
        [-0.1806],
        [ 0.0442],
        [-0.1379],
        [-0.2111],
        [-0.0166],
        [ 0.2288],
        [-0.2282],
        [ 0.0263],
        [ 0.0975],
        [ 0.0455],
        [-0.0844],
        [-0.0902],
        [ 0.0851],
        [-0.0849],
        [ 0.1022],
        [ 0.1443],
        [-0.1992],
        [ 0.2012],
        [-0.0416],
        [-0.0025],
        [ 0.1814],
        [-0.1046],
        [ 0.1025],
        [-0.0300],
        [-0.1404],
        [ 0.1619],
        [ 0.1946],
        [ 0.1949],
        [-0.0480],
        [ 0.0610],
        [ 0.0784],
        [ 0.0638],
        [-0.0775],
        [ 0.1184],
        [ 0.1963],
        [ 0.1184],
        [ 0.0908],
        [-0.1595],
        [ 0.1469],
        [ 0.0493],
        [ 0.1618],
        [-0.1418],
        [ 0.0625],
        [ 0.1334],
        [ 0.1531],
        [ 0.0266],
        [ 0.2001],
        [-0.0550],
        [ 0.0952],
        [-0.0774],
        [ 0.1990],
        [-0.1731],
        [ 0.1783],
        [ 0.2015],
        [ 0.1003],
        [-0.1680],
        [ 0.1334],
        [ 0.0959],
        [ 0.1093],
        [-0.1224],
        [ 0.0807],
        [-0.1885],
        [ 0.1549],
        [ 0.1238],
        [ 0.1434],
        [-0.2034],
        [ 0.0464],
        [-0.2106],
        [-0.0098],
        [-0.2333],
        [ 0.1023],
        [ 0.1999],
        [-0.1843],
        [ 0.0549],
        [ 0.0566],
        [-0.1140],
        [ 0.1194],
        [-0.0652],
        [ 0.1847],
        [-0.1292],
        [ 0.1218],
        [ 0.0620],
        [ 0.1887],
        [ 0.0575],
        [-0.1324],
        [-0.0604],
        [ 0.2016],
        [ 0.0108],
        [ 0.1119],
        [-0.1339],
        [ 0.2183],
        [-0.0243],
        [-0.0157],
        [-0.1074],
        [ 0.2196],
        [ 0.0789],
        [ 0.0780],
        [-0.1387],
        [ 0.0276],
        [-0.1681],
        [-0.0834],
        [ 0.0196],
        [-0.0178],
        [ 0.0812],
        [ 0.0978],
        [ 0.1745],
        [-0.1010],
        [ 0.1165],
        [ 0.1062],
        [ 0.1874],
        [ 0.0961],
        [ 0.0622],
        [-0.0447],
        [-0.0583],
        [ 0.0080],
        [ 0.0291],
        [-0.1445],
        [-0.0572],
        [-0.1511],
        [ 0.0091],
        [-0.1771],
        [-0.1993],
        [-0.1866],
        [-0.0269],
        [-0.0667],
        [-0.1857],
        [-0.1231],
        [-0.0032],
        [ 0.0946],
        [-0.0078],
        [-0.0858],
        [-0.1159],
        [ 0.1224],
        [-0.0270],
        [-0.1253],
        [-0.0960],
        [ 0.2130],
        [-0.0853],
        [ 0.0599],
        [-0.1944],
        [-0.1038],
        [ 0.0515],
        [-0.1401],
        [ 0.1542],
        [ 0.0544],
        [ 0.0678],
        [-0.2142],
        [ 0.1146],
        [-0.1038],
        [ 0.2021],
        [-0.0807],
        [-0.1922],
        [ 0.1556],
        [-0.2121],
        [ 0.0007],
        [ 0.2042],
        [ 0.0109],
        [-0.0247],
        [-0.0128],
        [ 0.1763],
        [ 0.0489],
        [ 0.1532],
        [ 0.1953],
        [-0.0686],
        [-0.0602],
        [-0.0041],
        [ 0.1030],
        [-0.2349],
        [ 0.0275],
        [ 0.1607],
        [-0.0227],
        [ 0.0458],
        [ 0.0662],
        [ 0.1553],
        [ 0.0277],
        [-0.1866],
        [ 0.1409],
        [-0.0392],
        [ 0.1814],
        [-0.0299],
        [-0.0436],
        [-0.1361],
        [-0.0878],
        [ 0.2119],
        [-0.1363],
        [ 0.1702],
        [-0.0055],
        [ 0.1329],
        [-0.1119],
        [ 0.0741],
        [ 0.1639],
        [-0.0096],
        [ 0.1307],
        [ 0.1926],
        [-0.0775],
        [-0.1872],
        [ 0.0059],
        [ 0.0949],
        [-0.2387],
        [ 0.0650],
        [ 0.2112],
        [-0.1758],
        [-0.0961],
        [ 0.1185],
        [ 0.0287],
        [-0.0112],
        [-0.1049],
        [ 0.0847],
        [ 0.1073],
        [-0.1076],
        [ 0.0723],
        [-0.2610],
        [ 0.0897],
        [-0.1856],
        [-0.1430],
        [-0.1250],
        [ 0.1701],
        [-0.0059],
        [-0.1712],
        [-0.0967],
        [-0.0477],
        [ 0.1866],
        [-0.1985],
        [-0.0032],
        [-0.1366],
        [-0.2322],
        [-0.1720],
        [ 0.0770],
        [-0.0821],
        [-0.0141],
        [ 0.1557],
        [ 0.2074],
        [ 0.1330],
        [ 0.1538],
        [-0.0520],
        [-0.0941],
        [ 0.1778],
        [ 0.2061]])), ('ConvLayer1_AttentionHead1.bias', tensor([-0.0104,  0.0119, -0.0013, -0.0178, -0.0273,  0.0034,  0.0368,  0.0119,
         0.0266,  0.0124, -0.0185,  0.0100, -0.0176, -0.0157, -0.0251, -0.0085,
        -0.0203,  0.0153, -0.0267, -0.0147, -0.0128,  0.0071, -0.0101, -0.0108,
         0.0164, -0.0189, -0.0215,  0.0175, -0.0059,  0.0198,  0.0030,  0.0170,
        -0.0061,  0.0011, -0.0087, -0.0188,  0.0129,  0.0089, -0.0038,  0.0094,
         0.0166, -0.0196, -0.0408,  0.0113, -0.0115, -0.0127, -0.0290, -0.0200,
        -0.0076, -0.0106, -0.0073, -0.0172, -0.0306,  0.0234, -0.0242,  0.0334,
         0.0177,  0.0170, -0.0205,  0.0231,  0.0153, -0.0109, -0.0260, -0.0106,
        -0.0058, -0.0266,  0.0096, -0.0235,  0.0077, -0.0125,  0.0195, -0.0131,
         0.0090,  0.0145,  0.0171,  0.0046,  0.0159,  0.0040, -0.0146,  0.0144,
        -0.0230, -0.0271,  0.0211,  0.0088,  0.0341,  0.0330, -0.0158,  0.0055,
         0.0434, -0.0090,  0.0138,  0.0063,  0.0134,  0.0127, -0.0257,  0.0220,
         0.0261, -0.0175,  0.0146,  0.0073, -0.0168,  0.0102,  0.0138,  0.0298,
         0.0089,  0.0214, -0.0211, -0.0205, -0.0089, -0.0053, -0.0213, -0.0055,
        -0.0090, -0.0093, -0.0157,  0.0435, -0.0239,  0.0248,  0.0379,  0.0121,
        -0.0046,  0.0024,  0.0106,  0.0089, -0.0292,  0.0249,  0.0044, -0.0176])), ('ConvLayer1_AttentionHead2.weight', tensor([[ 0.0583, -0.1668,  0.1561,  ..., -0.0759,  0.1234,  0.0184],
        [ 0.0603, -0.1554,  0.0020,  ..., -0.1129, -0.0415,  0.1034],
        [-0.0348, -0.0330,  0.1085,  ...,  0.1045, -0.0695, -0.1205],
        ...,
        [-0.1295, -0.1324, -0.1266,  ..., -0.0415,  0.1332, -0.1274],
        [ 0.0210, -0.0105, -0.0265,  ...,  0.0642, -0.1303, -0.0087],
        [ 0.0951,  0.0431, -0.0911,  ..., -0.1558, -0.0790,  0.0767]])), ('ConvLayer1_AttentionHead2.weight_interact', tensor([[ 0.1175,  0.0662, -0.1267,  ...,  0.0446,  0.0620, -0.1443],
        [-0.0998, -0.0255, -0.1287,  ...,  0.0035, -0.0131, -0.0145],
        [ 0.0804,  0.0901,  0.0390,  ...,  0.0707, -0.1040, -0.0025],
        ...,
        [ 0.1093,  0.0965,  0.1207,  ...,  0.0716, -0.0757, -0.0930],
        [ 0.0564,  0.0910, -0.1470,  ...,  0.1111,  0.1181, -0.0949],
        [-0.1070, -0.0180, -0.0317,  ...,  0.0195, -0.0874,  0.0261]])), ('ConvLayer1_AttentionHead2.a', tensor([[-2.7416e-02],
        [ 2.1440e-01],
        [ 6.5252e-02],
        [-2.5791e-01],
        [ 9.5042e-02],
        [ 1.7498e-01],
        [ 7.0564e-02],
        [-1.1419e-01],
        [ 1.2636e-01],
        [-2.0091e-01],
        [ 2.3895e-02],
        [ 5.1059e-02],
        [-2.0669e-01],
        [-4.1184e-02],
        [-7.4210e-02],
        [-7.2591e-02],
        [ 1.5697e-01],
        [ 8.6330e-02],
        [-3.5474e-02],
        [ 1.4463e-01],
        [-7.7739e-02],
        [-1.9726e-01],
        [ 6.5094e-02],
        [-1.9019e-01],
        [-1.7582e-01],
        [ 1.7846e-01],
        [-1.3162e-01],
        [-9.3989e-02],
        [ 7.4344e-02],
        [ 4.4379e-02],
        [-3.8645e-02],
        [ 1.2725e-01],
        [-1.7455e-01],
        [-4.0964e-02],
        [ 2.1168e-01],
        [-2.4609e-01],
        [-1.6361e-02],
        [-7.2283e-02],
        [-1.7294e-01],
        [-2.1482e-01],
        [ 1.5212e-01],
        [-5.4256e-02],
        [ 1.7601e-01],
        [-7.7105e-02],
        [-4.3805e-03],
        [-1.1828e-01],
        [ 1.4503e-01],
        [-8.0252e-02],
        [ 1.3383e-01],
        [-1.8473e-01],
        [ 7.1521e-02],
        [ 8.7590e-02],
        [ 1.7852e-01],
        [ 3.9567e-02],
        [-9.5221e-02],
        [-6.4791e-03],
        [ 8.7324e-02],
        [-1.5409e-01],
        [ 1.2999e-01],
        [ 5.3808e-02],
        [-1.1240e-01],
        [-1.8090e-01],
        [-1.8374e-01],
        [ 4.4150e-02],
        [ 1.7237e-01],
        [-1.6036e-01],
        [ 7.8383e-02],
        [ 8.5295e-02],
        [ 1.9373e-01],
        [-2.3370e-01],
        [ 1.7373e-01],
        [ 9.3391e-02],
        [-7.8657e-02],
        [-4.6310e-03],
        [ 5.3078e-02],
        [-9.6587e-02],
        [-4.8054e-02],
        [ 3.1781e-02],
        [ 1.8591e-01],
        [ 8.4435e-02],
        [-1.8354e-01],
        [ 1.4666e-02],
        [-9.9441e-02],
        [ 9.2958e-03],
        [-1.7303e-01],
        [-1.1162e-01],
        [ 2.2893e-01],
        [-1.0979e-01],
        [ 1.2141e-01],
        [ 8.7284e-02],
        [-2.2537e-01],
        [ 7.5086e-03],
        [ 1.4717e-01],
        [ 1.3072e-01],
        [ 1.7229e-02],
        [ 8.5130e-02],
        [ 6.5113e-02],
        [ 5.6967e-03],
        [-6.2568e-02],
        [-3.9323e-02],
        [ 7.3425e-02],
        [-2.0973e-01],
        [-4.4985e-02],
        [-7.2501e-02],
        [ 9.2345e-05],
        [ 1.6014e-01],
        [ 1.6593e-01],
        [ 1.5467e-02],
        [ 1.6342e-01],
        [-1.8375e-01],
        [-4.2345e-02],
        [ 1.0952e-03],
        [ 9.3575e-02],
        [ 7.5112e-02],
        [ 1.3636e-01],
        [ 5.2872e-02],
        [-5.4187e-02],
        [-1.6764e-01],
        [-1.5323e-01],
        [-2.9414e-02],
        [ 1.5742e-01],
        [ 1.4698e-01],
        [-1.3604e-01],
        [-5.5922e-02],
        [-1.6256e-01],
        [ 1.4303e-01],
        [ 1.7688e-01],
        [ 9.6229e-02],
        [-1.5044e-01],
        [ 8.7033e-02],
        [-8.1138e-02],
        [ 1.2695e-01],
        [-1.0159e-01],
        [ 4.1062e-02],
        [-5.3142e-02],
        [ 1.5053e-02],
        [ 1.0121e-01],
        [ 1.5200e-01],
        [ 1.2418e-01],
        [-5.0545e-02],
        [ 1.6022e-01],
        [ 2.0667e-01],
        [ 1.9270e-01],
        [-1.0375e-01],
        [ 1.8574e-01],
        [ 8.5618e-02],
        [-4.0607e-02],
        [-8.1672e-02],
        [-5.0298e-02],
        [ 1.7124e-01],
        [-8.8674e-02],
        [-2.0059e-01],
        [ 1.8338e-01],
        [-1.0579e-01],
        [ 8.2109e-02],
        [ 1.0614e-01],
        [ 2.1824e-01],
        [-1.8061e-01],
        [ 7.3686e-02],
        [-1.1770e-01],
        [ 4.2635e-03],
        [ 4.6146e-02],
        [-6.7116e-02],
        [ 1.7114e-01],
        [ 1.2131e-01],
        [ 7.8087e-02],
        [ 9.3035e-02],
        [-1.2739e-01],
        [ 1.2849e-01],
        [ 1.0160e-01],
        [-1.8407e-01],
        [ 1.2393e-01],
        [ 1.9761e-01],
        [ 2.7908e-02],
        [ 1.5627e-01],
        [-3.5442e-02],
        [-7.0832e-02],
        [-3.8374e-02],
        [-2.2436e-02],
        [ 1.5245e-01],
        [-1.8566e-01],
        [ 9.9191e-02],
        [-1.2185e-01],
        [ 9.3586e-02],
        [ 6.8503e-02],
        [ 8.2442e-02],
        [ 1.6169e-01],
        [ 1.9072e-01],
        [ 5.2297e-02],
        [-5.1763e-02],
        [-1.5175e-02],
        [-9.9114e-02],
        [ 3.0541e-02],
        [-5.5481e-02],
        [ 1.2907e-01],
        [ 3.3067e-02],
        [ 6.7274e-02],
        [ 1.5300e-01],
        [ 5.2065e-02],
        [ 1.1106e-01],
        [-1.3242e-01],
        [ 1.7325e-01],
        [-1.9709e-01],
        [ 8.1755e-02],
        [-4.6715e-02],
        [-2.8406e-02],
        [-9.8659e-03],
        [-1.9910e-01],
        [-6.0338e-02],
        [-4.5697e-02],
        [-1.2126e-01],
        [-1.8509e-01],
        [ 2.8799e-02],
        [ 1.9556e-01],
        [-1.2801e-01],
        [-1.1491e-01],
        [-1.6700e-01],
        [-1.1415e-01],
        [-8.5021e-02],
        [ 1.9926e-01],
        [ 5.0243e-02],
        [ 2.9635e-03],
        [-1.9650e-01],
        [ 1.6813e-01],
        [ 1.0349e-01],
        [ 9.8080e-02],
        [-8.8676e-02],
        [ 4.7046e-03],
        [ 4.4058e-03],
        [ 6.4109e-03],
        [-8.7829e-02],
        [ 8.5655e-02],
        [-1.2958e-01],
        [-7.1584e-02],
        [ 1.4503e-02],
        [-1.4652e-01],
        [-1.1121e-01],
        [-9.5443e-02],
        [-1.6258e-01],
        [ 1.7727e-01],
        [-1.6292e-01],
        [ 1.8344e-02],
        [ 1.7459e-01],
        [ 4.1017e-02],
        [ 6.0469e-02],
        [-2.2022e-01],
        [-2.0594e-01],
        [ 8.8887e-02],
        [-3.7300e-03],
        [ 6.6108e-03],
        [ 1.7002e-01],
        [ 1.7638e-01],
        [-9.9370e-02],
        [ 6.2916e-02],
        [ 2.0287e-01],
        [-1.5539e-01]])), ('ConvLayer1_AttentionHead2.bias', tensor([-0.0017, -0.0080,  0.0096,  0.0085,  0.0214, -0.0090,  0.0065,  0.0009,
        -0.0056, -0.0172,  0.0059,  0.0238,  0.0051, -0.0312,  0.0151,  0.0022,
        -0.0309, -0.0177,  0.0159,  0.0198, -0.0237,  0.0129,  0.0151,  0.0377,
        -0.0034,  0.0073,  0.0154, -0.0288, -0.0186, -0.0018,  0.0151, -0.0035,
        -0.0094, -0.0277, -0.0178, -0.0019,  0.0139, -0.0186, -0.0085,  0.0250,
        -0.0240,  0.0066,  0.0059, -0.0276, -0.0236, -0.0201,  0.0085,  0.0070,
         0.0168,  0.0350,  0.0109, -0.0159,  0.0095, -0.0157, -0.0430, -0.0292,
        -0.0180, -0.0369, -0.0263,  0.0143,  0.0095,  0.0117, -0.0168, -0.0086,
         0.0270,  0.0012, -0.0070, -0.0100,  0.0096, -0.0223, -0.0231, -0.0433,
        -0.0021,  0.0238,  0.0101, -0.0238,  0.0235,  0.0154, -0.0077, -0.0150,
        -0.0091, -0.0006,  0.0186,  0.0277,  0.0077,  0.0062, -0.0257,  0.0138,
        -0.0073, -0.0105,  0.0201,  0.0037, -0.0195, -0.0196,  0.0308, -0.0383,
        -0.0338, -0.0007,  0.0133, -0.0220,  0.0174,  0.0183,  0.0176,  0.0013,
        -0.0162, -0.0344, -0.0291, -0.0099,  0.0178,  0.0242, -0.0066, -0.0124,
        -0.0222, -0.0207, -0.0294,  0.0117, -0.0198, -0.0097, -0.0013, -0.0261,
         0.0241, -0.0221,  0.0195,  0.0168,  0.0087,  0.0096, -0.0254, -0.0188])), ('ConvLayer2_AttentionHead0.weight', tensor([[ 0.0519, -0.0870, -0.1174,  ..., -0.0677,  0.1114,  0.0270],
        [ 0.0469,  0.0877,  0.0888,  ..., -0.0844,  0.1422, -0.0757],
        [ 0.0472, -0.0470, -0.0834,  ..., -0.0243,  0.0595, -0.1113],
        ...,
        [ 0.0393,  0.1035, -0.1836,  ...,  0.0708,  0.0065,  0.0266],
        [-0.0248,  0.0081, -0.1126,  ..., -0.0154,  0.0457,  0.0076],
        [-0.0728,  0.1755, -0.1133,  ..., -0.0574,  0.1685, -0.0032]])), ('ConvLayer2_AttentionHead0.weight_interact', tensor([[-0.0099,  0.0077,  0.0283,  ...,  0.0901,  0.0084, -0.0230],
        [-0.1542,  0.1376, -0.0551,  ...,  0.1053,  0.0771,  0.0707],
        [-0.1603,  0.0633, -0.0076,  ..., -0.0194,  0.0895, -0.1171],
        ...,
        [ 0.0339,  0.0568, -0.1301,  ..., -0.1555, -0.1055, -0.0874],
        [-0.0601,  0.0362,  0.1529,  ..., -0.1143, -0.0799,  0.1237],
        [ 0.1492, -0.1601,  0.1021,  ..., -0.1468,  0.0186,  0.0895]])), ('ConvLayer2_AttentionHead0.a', tensor([[-0.0865],
        [-0.2039],
        [-0.2092],
        [-0.3201],
        [ 0.1942],
        [-0.1077],
        [ 0.1927],
        [-0.0720],
        [ 0.0425],
        [-0.2587],
        [-0.2592],
        [-0.0392],
        [ 0.2759],
        [ 0.2845],
        [ 0.2034],
        [-0.3154],
        [-0.0007],
        [ 0.3004],
        [ 0.1205],
        [ 0.1759],
        [ 0.0217],
        [ 0.1796],
        [ 0.3365],
        [-0.4118],
        [ 0.3162],
        [-0.2497],
        [ 0.1180],
        [-0.1898],
        [-0.2118],
        [ 0.2275],
        [-0.2277],
        [ 0.0974],
        [-0.3150],
        [-0.1556],
        [ 0.2173],
        [ 0.2357],
        [-0.1651],
        [-0.3076],
        [-0.0694],
        [ 0.3011],
        [-0.1216],
        [ 0.1636],
        [ 0.2176],
        [ 0.2834],
        [-0.1827],
        [ 0.3094],
        [ 0.0636],
        [-0.2249],
        [ 0.2473],
        [-0.0402],
        [ 0.0033],
        [ 0.2605],
        [-0.1060],
        [ 0.1279],
        [-0.0763],
        [ 0.0482],
        [ 0.2375],
        [ 0.1720],
        [-0.3236],
        [ 0.2650],
        [-0.2395],
        [ 0.0916],
        [ 0.2411],
        [ 0.2798],
        [ 0.0244],
        [-0.0838],
        [ 0.2331],
        [-0.3304],
        [-0.1702],
        [-0.0099],
        [ 0.0424],
        [ 0.0636],
        [-0.2205],
        [ 0.0292],
        [ 0.0963],
        [-0.0439],
        [-0.2337],
        [ 0.1421],
        [ 0.1202],
        [-0.3274],
        [-0.0597],
        [ 0.0932],
        [-0.2476],
        [-0.2648],
        [-0.2031],
        [ 0.0788],
        [-0.1546],
        [-0.0860],
        [-0.0147],
        [-0.2713],
        [-0.1376],
        [-0.1905],
        [-0.0650],
        [-0.1933],
        [ 0.2789],
        [ 0.1421],
        [ 0.0939],
        [-0.1609],
        [ 0.0831],
        [-0.2827],
        [-0.2564],
        [-0.3423],
        [ 0.1740],
        [-0.3805],
        [-0.1974],
        [-0.1648],
        [-0.1701],
        [-0.1619],
        [-0.0998],
        [ 0.1387],
        [-0.0912],
        [ 0.0825],
        [ 0.2401],
        [-0.2542],
        [-0.2576],
        [ 0.1207],
        [ 0.0012],
        [ 0.1837],
        [-0.0080],
        [-0.2710],
        [-0.1180],
        [-0.3372],
        [-0.3390],
        [ 0.0038],
        [ 0.0639],
        [ 0.0112],
        [ 0.0857],
        [ 0.1103]])), ('ConvLayer2_AttentionHead0.bias', tensor([-0.0928, -0.1103, -0.0713, -0.0046,  0.0091,  0.1241,  0.0240, -0.0984,
         0.0977, -0.0346, -0.1182,  0.0759,  0.0384, -0.0317,  0.0432, -0.0803,
        -0.0150,  0.0246,  0.0858,  0.0637,  0.0222,  0.0806,  0.0975, -0.0189,
         0.1223,  0.0873, -0.0255, -0.0121, -0.0275,  0.0803,  0.0845,  0.0711,
        -0.1386, -0.1053, -0.0382,  0.0184,  0.0691, -0.0542,  0.0739,  0.0573,
         0.0355, -0.0245, -0.0573,  0.1016,  0.0262,  0.0887,  0.0611, -0.1233,
         0.1010, -0.0248,  0.0039, -0.0316, -0.1029, -0.1488,  0.1339,  0.0955,
         0.1015, -0.1122,  0.1053,  0.0989, -0.0399, -0.1181,  0.0893,  0.0122])), ('ConvLayer2_AttentionHead1.weight', tensor([[-0.0023,  0.0089,  0.0235,  ...,  0.0739,  0.0242,  0.0279],
        [-0.0106,  0.1047, -0.1905,  ..., -0.1789,  0.0967, -0.0122],
        [-0.1338, -0.1144, -0.0182,  ..., -0.0687, -0.0209,  0.1584],
        ...,
        [ 0.1390, -0.1905,  0.0343,  ...,  0.0360, -0.0119,  0.0965],
        [ 0.0795, -0.0002,  0.0891,  ...,  0.0877,  0.0708, -0.1323],
        [ 0.1061,  0.0219, -0.1356,  ...,  0.0852,  0.1305,  0.0777]])), ('ConvLayer2_AttentionHead1.weight_interact', tensor([[ 0.0721, -0.0640,  0.0089,  ...,  0.0510, -0.1588,  0.1621],
        [ 0.1293,  0.1625,  0.1171,  ..., -0.0378,  0.1628,  0.0805],
        [-0.0506, -0.0532,  0.0989,  ...,  0.0048, -0.0034,  0.1246],
        ...,
        [-0.0300,  0.0317,  0.0692,  ...,  0.0442, -0.0379, -0.0198],
        [ 0.0251, -0.0493, -0.0917,  ..., -0.0407, -0.1186, -0.0442],
        [-0.0664, -0.0789,  0.0865,  ..., -0.1402,  0.1198, -0.0531]])), ('ConvLayer2_AttentionHead1.a', tensor([[-2.7928e-01],
        [-1.5049e-01],
        [ 2.3033e-01],
        [-1.7883e-01],
        [-2.1422e-01],
        [-1.8913e-01],
        [ 1.4007e-01],
        [-2.5645e-01],
        [ 2.4377e-01],
        [ 9.3768e-02],
        [ 1.8226e-02],
        [-6.7320e-02],
        [ 1.2043e-01],
        [-3.0587e-02],
        [-1.3036e-01],
        [ 4.7362e-02],
        [-8.6756e-02],
        [ 1.4920e-01],
        [ 1.4599e-01],
        [ 2.7976e-01],
        [-1.9960e-01],
        [-2.6613e-02],
        [ 2.5918e-01],
        [ 9.9235e-02],
        [-8.7966e-02],
        [ 2.0284e-01],
        [-8.0670e-02],
        [-2.1069e-01],
        [-2.3536e-01],
        [ 1.1337e-01],
        [ 2.4603e-01],
        [ 1.3781e-01],
        [ 1.3848e-01],
        [ 2.3779e-01],
        [ 9.4716e-02],
        [-2.4840e-01],
        [ 8.9500e-04],
        [ 2.9795e-01],
        [ 1.4748e-01],
        [-2.2362e-01],
        [ 5.5812e-02],
        [-1.8730e-01],
        [ 1.1211e-01],
        [-1.2409e-01],
        [-6.2183e-02],
        [-8.4094e-03],
        [ 1.3757e-01],
        [-2.3000e-01],
        [ 3.7174e-02],
        [ 4.3417e-02],
        [ 3.5496e-01],
        [ 2.7009e-01],
        [-1.6015e-01],
        [-2.5727e-01],
        [ 9.8979e-02],
        [-1.9480e-01],
        [-2.8107e-01],
        [ 2.7427e-01],
        [-3.0207e-01],
        [-1.2693e-01],
        [-2.6970e-01],
        [-2.0784e-01],
        [-2.3262e-01],
        [-2.3051e-01],
        [ 3.0701e-01],
        [ 2.1989e-01],
        [-2.0419e-01],
        [ 4.5612e-02],
        [ 1.1813e-02],
        [-7.8301e-02],
        [-3.6163e-02],
        [ 2.1314e-02],
        [ 2.6225e-01],
        [ 1.3233e-01],
        [ 5.8758e-02],
        [ 1.5157e-01],
        [-1.9982e-01],
        [ 2.1349e-01],
        [-3.4023e-04],
        [-3.4690e-02],
        [-1.2448e-01],
        [ 1.6541e-01],
        [ 7.4940e-02],
        [ 1.4464e-01],
        [-1.1402e-01],
        [ 3.6790e-02],
        [-2.4130e-01],
        [ 2.2454e-01],
        [ 1.5922e-01],
        [ 2.2935e-01],
        [-7.5206e-02],
        [-2.8672e-01],
        [ 3.1413e-01],
        [-1.0428e-01],
        [ 1.6936e-01],
        [ 1.3195e-02],
        [ 1.7945e-01],
        [-5.0375e-02],
        [-2.3007e-01],
        [ 3.5859e-01],
        [-2.8317e-01],
        [ 5.5503e-02],
        [ 2.8066e-01],
        [-1.0209e-01],
        [ 1.8125e-01],
        [ 8.3401e-02],
        [ 4.0773e-02],
        [ 1.9465e-01],
        [ 5.9157e-02],
        [ 2.5260e-01],
        [-2.0501e-03],
        [ 1.2184e-01],
        [-1.6032e-02],
        [-3.7783e-02],
        [ 2.1506e-01],
        [ 1.1944e-02],
        [ 1.8937e-01],
        [ 3.9552e-01],
        [ 2.3167e-01],
        [ 9.6286e-02],
        [-1.8286e-01],
        [-1.7683e-01],
        [ 3.0342e-01],
        [ 2.2281e-01],
        [-2.3855e-01],
        [ 5.2238e-02],
        [-3.1864e-01],
        [ 2.0556e-01]])), ('ConvLayer2_AttentionHead1.bias', tensor([-0.0928, -0.1103, -0.0713, -0.0046,  0.0091,  0.1241,  0.0240, -0.0984,
         0.0977, -0.0346, -0.1182,  0.0759,  0.0384, -0.0317,  0.0432, -0.0803,
        -0.0150,  0.0246,  0.0858,  0.0637,  0.0222,  0.0806,  0.0975, -0.0189,
         0.1223,  0.0873, -0.0255, -0.0121, -0.0275,  0.0803,  0.0845,  0.0711,
        -0.1386, -0.1053, -0.0382,  0.0184,  0.0691, -0.0542,  0.0739,  0.0573,
         0.0355, -0.0245, -0.0573,  0.1016,  0.0262,  0.0887,  0.0611, -0.1233,
         0.1010, -0.0248,  0.0039, -0.0316, -0.1029, -0.1488,  0.1339,  0.0955,
         0.1015, -0.1122,  0.1053,  0.0989, -0.0399, -0.1181,  0.0893,  0.0122])), ('ConvLayer2_AttentionHead2.weight', tensor([[ 0.0777,  0.0423, -0.1540,  ..., -0.1237, -0.0850,  0.1779],
        [-0.1395,  0.0663, -0.1594,  ..., -0.1710,  0.1445, -0.0376],
        [-0.1832,  0.0637, -0.0788,  ..., -0.0121,  0.0097, -0.1144],
        ...,
        [-0.0512,  0.0552,  0.1356,  ..., -0.0985,  0.0918,  0.0737],
        [ 0.0255,  0.1361, -0.1491,  ..., -0.1016,  0.0748, -0.0711],
        [-0.0333, -0.1318,  0.1221,  ..., -0.0042, -0.0547, -0.0207]])), ('ConvLayer2_AttentionHead2.weight_interact', tensor([[-0.0863,  0.1296,  0.0077,  ...,  0.0682,  0.0923, -0.0942],
        [ 0.0520, -0.0633, -0.0843,  ..., -0.0385,  0.1074,  0.0086],
        [ 0.1474, -0.0079, -0.0632,  ...,  0.0176, -0.0240, -0.0045],
        ...,
        [ 0.1272, -0.0312,  0.0610,  ...,  0.0505,  0.0551, -0.1539],
        [-0.0669, -0.0456, -0.1062,  ...,  0.1439, -0.1386,  0.0545],
        [-0.0152,  0.0507, -0.0465,  ...,  0.1139,  0.1613, -0.1198]])), ('ConvLayer2_AttentionHead2.a', tensor([[ 0.1357],
        [-0.1662],
        [-0.2236],
        [ 0.0518],
        [ 0.0499],
        [ 0.2210],
        [ 0.2803],
        [-0.1376],
        [ 0.2590],
        [-0.1867],
        [-0.2687],
        [ 0.3425],
        [-0.2554],
        [-0.2397],
        [ 0.1162],
        [-0.3295],
        [-0.1949],
        [ 0.3255],
        [ 0.2337],
        [-0.0659],
        [ 0.0864],
        [ 0.2847],
        [ 0.2301],
        [-0.2130],
        [-0.1024],
        [-0.2309],
        [-0.0359],
        [ 0.0369],
        [ 0.2882],
        [-0.0667],
        [ 0.1015],
        [ 0.0363],
        [-0.3026],
        [-0.1456],
        [-0.2585],
        [ 0.2258],
        [ 0.0964],
        [ 0.0631],
        [-0.2527],
        [ 0.2908],
        [-0.2943],
        [-0.0521],
        [ 0.0481],
        [ 0.3305],
        [-0.1345],
        [ 0.1838],
        [-0.2169],
        [-0.1386],
        [ 0.2546],
        [ 0.0652],
        [-0.0833],
        [ 0.0444],
        [-0.0639],
        [-0.1361],
        [ 0.1486],
        [ 0.1919],
        [ 0.3049],
        [-0.1534],
        [-0.1141],
        [-0.0284],
        [ 0.0471],
        [-0.2902],
        [ 0.0287],
        [ 0.3676],
        [ 0.1551],
        [ 0.0767],
        [ 0.0772],
        [ 0.1883],
        [ 0.1856],
        [-0.2728],
        [ 0.1619],
        [-0.2406],
        [ 0.0998],
        [ 0.0321],
        [ 0.2212],
        [ 0.0954],
        [-0.0933],
        [ 0.2662],
        [-0.3086],
        [-0.1079],
        [-0.1441],
        [-0.1734],
        [ 0.0449],
        [-0.2795],
        [-0.2604],
        [ 0.3241],
        [ 0.0191],
        [-0.2001],
        [ 0.1653],
        [-0.1913],
        [-0.2308],
        [-0.1393],
        [ 0.1919],
        [ 0.0113],
        [-0.0354],
        [ 0.1714],
        [ 0.3526],
        [ 0.2540],
        [ 0.2081],
        [-0.0753],
        [-0.0659],
        [ 0.0150],
        [-0.2945],
        [ 0.1240],
        [-0.0347],
        [-0.1430],
        [ 0.0047],
        [ 0.2382],
        [ 0.1492],
        [-0.3350],
        [ 0.0805],
        [-0.1604],
        [ 0.2983],
        [-0.2500],
        [ 0.1704],
        [-0.0305],
        [ 0.2317],
        [-0.1573],
        [-0.1508],
        [ 0.0504],
        [-0.3180],
        [-0.2815],
        [-0.3028],
        [-0.3079],
        [-0.1939],
        [-0.1323],
        [-0.0017],
        [ 0.1651]])), ('ConvLayer2_AttentionHead2.bias', tensor([-0.0928, -0.1103, -0.0713, -0.0046,  0.0091,  0.1241,  0.0240, -0.0984,
         0.0977, -0.0346, -0.1182,  0.0759,  0.0384, -0.0317,  0.0432, -0.0803,
        -0.0150,  0.0246,  0.0858,  0.0637,  0.0222,  0.0806,  0.0975, -0.0189,
         0.1223,  0.0873, -0.0255, -0.0121, -0.0275,  0.0803,  0.0845,  0.0711,
        -0.1386, -0.1053, -0.0382,  0.0184,  0.0691, -0.0542,  0.0739,  0.0573,
         0.0355, -0.0245, -0.0573,  0.1016,  0.0262,  0.0887,  0.0611, -0.1233,
         0.1010, -0.0248,  0.0039, -0.0316, -0.1029, -0.1488,  0.1339,  0.0955,
         0.1015, -0.1122,  0.1053,  0.0989, -0.0399, -0.1181,  0.0893,  0.0122])), ('tf_linear1.weight', tensor([[-0.2964,  0.1534, -0.1606,  ...,  0.1074, -0.1817, -0.1047],
        [ 0.2542, -0.2065, -0.0571,  ...,  0.2588,  0.2491, -0.3790],
        [-0.3370, -0.1828, -0.4419,  ..., -0.5327, -0.0491,  0.0938],
        ...,
        [ 0.1182,  0.1277,  0.1125,  ...,  0.1925,  0.2503,  0.2180],
        [-0.0290, -0.2891,  0.2580,  ...,  0.1015, -0.1283,  0.0496],
        [-0.3680,  0.0359, -0.2503,  ..., -0.2000, -0.1922, -0.0566]])), ('tf_linear1.bias', tensor([-0.0647, -0.0417,  0.2536,  0.0776,  0.0798,  0.0264,  0.0716,  0.0563,
        -0.0964,  0.0594,  0.0927,  0.1044,  0.0943, -0.0723,  0.0125, -0.0912,
        -0.0621,  0.0737,  0.1061,  0.1777, -0.1607,  0.0538, -0.0418, -0.1158,
         0.0956,  0.0304,  0.0971, -0.1242, -0.1626, -0.0960, -0.0435,  0.1200])), ('target_linear1.weight', tensor([[ 0.1009,  0.3139,  0.2246,  ..., -0.1910, -0.0553, -0.2586],
        [ 0.1522,  0.3020,  0.2890,  ...,  0.1933, -0.2037,  0.0161],
        [-0.2575, -0.3248,  0.0061,  ..., -0.3292, -0.2581, -0.0377],
        ...,
        [-0.0354, -0.3197, -0.1403,  ...,  0.0503, -0.0148,  0.1117],
        [-0.1501, -0.2715,  0.1276,  ..., -0.3131, -0.0596,  0.1228],
        [-0.0326,  0.1462, -0.4685,  ..., -0.1575,  0.3395, -0.2777]])), ('target_linear1.bias', tensor([-0.0131, -0.2001,  0.0270, -0.1656, -0.0144, -0.0477,  0.1479,  0.1037,
        -0.0135,  0.0098, -0.0487,  0.1887, -0.0051, -0.1162,  0.0796, -0.0476,
         0.0957,  0.0588,  0.0948,  0.2191,  0.0152, -0.0555, -0.0807, -0.0984,
        -0.0760,  0.0926,  0.0721,  0.1658, -0.1915,  0.1587,  0.1104,  0.1345])), ('tf_linear2.weight', tensor([[-0.0492,  0.2796,  0.5943,  0.4557, -0.4098,  0.0403,  0.2556,  0.4338,
         -0.5001, -0.0938, -0.1301, -0.2603, -0.1613, -0.0801,  0.4925,  0.2075,
         -0.4428, -0.1642, -0.5342,  0.3091, -0.3652, -0.3955, -0.0116, -0.0137,
         -0.0957, -0.1009, -0.0659, -0.4843, -0.4585, -0.1653, -0.0165,  0.6626],
        [ 0.5277, -0.0828,  0.3353, -0.3580, -0.0998, -0.5211, -0.1027, -0.1430,
          0.0328,  0.1946,  0.1627,  0.3312, -0.1545,  0.1972,  0.2769, -0.1595,
          0.1380, -0.4698, -0.2366, -0.4471,  0.2774, -0.1524, -0.2788,  0.4113,
          0.2801, -0.1768,  0.1019, -0.3408, -0.3583, -0.1745,  0.1788,  0.3025],
        [ 0.3552, -0.2430,  0.1966, -0.6677, -0.3619, -0.5081,  0.0502,  0.2241,
          0.3565,  0.0319,  0.1647,  0.4506,  0.3573,  0.0998, -0.3422, -0.0263,
          0.5389, -0.6390,  0.0437, -0.0730,  0.2102,  0.2065, -0.0848,  0.4410,
          0.4584, -0.2572, -0.2037,  0.3238,  0.0127,  0.1646, -0.0345, -0.0790],
        [ 0.1054,  0.2395, -0.1430, -0.3024, -0.3687,  0.0071,  0.5047,  0.2290,
          0.1248, -0.0342,  0.3129, -0.2932,  0.1687,  0.4373,  0.2835,  0.0669,
          0.4100,  0.0086, -0.1891,  0.2293, -0.1028,  0.4024, -0.4647,  0.3242,
          0.1277,  0.4924,  0.2821, -0.2145, -0.0621,  0.3126,  0.3053,  0.4892],
        [-0.2221, -0.3678,  0.6696,  0.2839, -0.1215, -0.2190,  0.4379, -0.0065,
         -0.3962, -0.0868,  0.1910, -0.5577, -0.1784, -0.1884, -0.0976, -0.6028,
         -0.4035, -0.2144, -0.0858,  0.7467,  0.0183,  0.5155, -0.4484,  0.2499,
         -0.3810,  0.3787,  0.5812,  0.0607, -0.2451, -0.3214, -0.4788, -0.0390],
        [ 0.2999,  0.3957, -0.5893, -0.4835,  0.1138,  0.0941, -0.2093, -0.2901,
          0.0857, -0.2907,  0.3356,  0.2432,  0.0146,  0.3729,  0.2996, -0.4337,
          0.1450, -0.1833,  0.2711,  0.0449, -0.1043, -0.1247, -0.0047, -0.3199,
          0.5159, -0.2679, -0.1859, -0.3712,  0.2274, -0.0202,  0.2538,  0.2384],
        [-0.3723, -0.2563,  0.7188,  0.6230, -0.2616,  0.2439,  0.1989,  0.6373,
          0.4120, -0.2065,  0.6889,  0.0173, -0.4690, -0.4641, -0.2300, -0.3633,
         -0.3913,  0.4293, -0.0948,  0.3298, -0.5053, -0.2458, -0.1690, -0.2205,
          0.3647,  0.1084,  0.6886,  0.2058, -0.3904, -0.0256,  0.2353,  0.0250],
        [ 0.0709, -0.2886, -0.2271,  0.2797,  0.0677, -0.2714, -0.3216,  0.3713,
          0.4563, -0.2661,  0.3394,  0.3093,  0.4097,  0.1345,  0.1945,  0.2241,
          0.0594,  0.1854, -0.2990, -0.2544,  0.0732, -0.1190,  0.2144,  0.0585,
          0.4732, -0.2224,  0.2878,  0.2509, -0.1181, -0.1040, -0.4102, -0.1241],
        [ 0.4118,  0.1701, -0.0145,  0.0212, -0.1011,  0.2027,  0.1135,  0.4321,
         -0.2785, -0.3616, -0.2912, -0.1992,  0.1870,  0.2103, -0.4436,  0.2144,
          0.2659,  0.2038, -0.4321,  0.3750,  0.1859, -0.4220,  0.4464, -0.1121,
          0.4171,  0.1818,  0.1460,  0.4215,  0.4413,  0.2418,  0.2515,  0.4366],
        [ 0.3290,  0.0393, -0.2660,  0.1702, -0.2606,  0.2070, -0.6253, -0.1423,
          0.1614, -0.4883,  0.0953,  0.0785, -0.1391, -0.0620, -0.5084,  0.3580,
          0.3596, -0.2889, -0.4596, -0.2545,  0.2040, -0.3465, -0.1476,  0.0638,
         -0.1785, -0.5421, -0.1421, -0.0740, -0.1354,  0.2000, -0.2589, -0.1236],
        [-0.3558, -0.4729,  0.2421,  0.3821, -0.4765,  0.1111, -0.0960,  0.2209,
          0.1488,  0.1771,  0.4335,  0.3769,  0.3306, -0.4555,  0.4133, -0.2699,
         -0.0837, -0.1758, -0.2154,  0.6632, -0.5038,  0.1507,  0.0634,  0.0462,
          0.1672,  0.6184,  0.0498, -0.5085, -0.1393,  0.0514,  0.3196,  0.2428],
        [ 0.3778, -0.1188,  0.3493,  0.1883,  0.3824,  0.4684,  0.3931,  0.3197,
          0.1192,  0.3833,  0.5621,  0.3835,  0.3603,  0.2502,  0.3309,  0.4508,
         -0.2692,  0.0468,  0.1856,  0.4216, -0.3927,  0.0639, -0.5418,  0.5776,
          0.1176,  0.3535, -0.3938, -0.2370,  0.1839, -0.0861,  0.0823, -0.1818],
        [-0.2806,  0.2197, -0.3440, -0.3678,  0.0335, -0.3108,  0.1913, -0.6155,
          0.0923, -0.1610, -0.0626, -0.2598, -0.4274, -0.3493, -0.2557, -0.3601,
          0.4672, -0.6376,  0.2611,  0.0025,  0.4993,  0.0810,  0.1037, -0.1792,
         -0.1347, -0.5320, -0.7581,  0.6361,  0.0287, -0.0114, -0.1821, -0.3775],
        [ 0.0337,  0.2802, -0.1034, -0.1632, -0.3240, -0.1589, -0.2515, -0.1866,
          0.3116,  0.4118,  0.0918,  0.0587,  0.4710,  0.1828,  0.0810,  0.4468,
          0.2834,  0.5746,  0.3164,  0.2389,  0.0455,  0.1429,  0.1159, -0.3240,
         -0.0075, -0.0121,  0.4503,  0.1166,  0.2826, -0.0088,  0.4506,  0.6297],
        [ 0.5958,  0.5114, -0.1845, -0.6256, -0.2448, -0.3181, -0.3852,  0.2417,
          0.3142,  0.1570, -0.0787,  0.3645, -0.1678,  0.4464, -0.6624,  0.2257,
          0.4103, -0.5808,  0.3764, -0.5949, -0.0369,  0.2559,  0.5681, -0.0601,
         -0.1549, -0.2220,  0.0626,  0.2192,  0.3869,  0.1745,  0.5777, -0.6362],
        [ 0.5821,  0.1305, -0.7040, -0.5526,  0.6207, -0.3885, -0.4722, -0.3411,
         -0.3467,  0.2804,  0.2683,  0.3279,  0.0765,  0.6184, -0.5192,  0.4550,
          0.1723, -0.6428,  0.0463,  0.0244, -0.2977, -0.3520,  0.3944, -0.2921,
          0.0463,  0.0592, -0.4281, -0.0053,  0.6000,  0.5722,  0.3416, -0.6418]])), ('tf_linear2.bias', tensor([ 0.1028, -0.2006,  0.0173, -0.0887,  0.1087, -0.2317,  0.2254, -0.1485,
         0.0928, -0.0768,  0.2492,  0.1494, -0.1155,  0.1197, -0.3461, -0.2394])), ('target_linear2.weight', tensor([[ 3.0286e-02,  2.6707e-01,  1.1232e-02,  3.0633e-01, -5.0751e-01,
          1.0694e-01, -4.5657e-01,  4.9779e-01, -1.0617e-01,  5.8063e-02,
          1.8338e-01, -5.8416e-01,  4.0376e-01,  4.2979e-01, -6.9408e-01,
         -2.0399e-01, -6.8355e-01, -3.9442e-01, -5.3628e-01, -5.9103e-01,
          1.0760e-01,  1.5012e-01,  2.8608e-01, -1.3534e-01, -1.1221e-01,
         -4.0911e-01,  1.4796e-01, -5.5850e-01,  2.6222e-01, -5.1627e-02,
          1.9973e-01,  1.7025e-01],
        [ 4.2452e-01,  8.7688e-02,  3.1175e-01,  1.5815e-01, -2.3525e-01,
         -3.1969e-01, -4.1427e-01, -4.0272e-01,  1.3667e-01, -2.1808e-02,
         -8.8898e-02,  8.9313e-02, -2.1488e-01, -2.7388e-01,  4.8471e-01,
          3.7552e-01,  2.5409e-01,  1.3005e-01, -2.2583e-01, -1.8165e-01,
          2.3287e-01, -2.6213e-01, -1.0443e-01, -2.2271e-01, -1.2710e-01,
         -5.2645e-01,  3.4094e-01,  1.3435e-01, -1.8874e-01,  3.7768e-01,
          4.4401e-02, -5.6638e-02],
        [-2.8595e-01,  3.2422e-01,  4.0216e-01, -4.1488e-01,  3.4744e-03,
          4.1889e-01, -6.0882e-02, -6.5309e-02,  2.3084e-01, -3.9886e-01,
         -5.9019e-01,  3.5238e-01, -4.3442e-01,  4.8007e-01, -1.8181e-01,
          3.3809e-01, -5.0564e-01,  5.2941e-01,  8.1738e-02,  4.8639e-01,
         -4.8221e-01,  2.1506e-01,  4.2315e-01,  1.8442e-01,  2.3174e-01,
          4.7055e-02,  1.2658e-01,  3.4894e-02, -2.8107e-01, -1.2437e-01,
         -5.3879e-02,  4.1460e-02],
        [ 2.0308e-01, -2.1582e-01, -9.4061e-02,  2.6222e-01, -4.4372e-01,
          8.1829e-02, -3.4539e-01, -8.6411e-02, -1.1116e-01,  4.3476e-01,
         -2.6173e-01, -5.6639e-01,  1.9107e-01,  5.4263e-01, -3.2675e-01,
         -3.9360e-01,  2.4474e-01,  2.9004e-01, -5.1271e-01, -3.1276e-01,
          1.2884e-01, -2.8565e-02, -1.6419e-01, -4.0302e-01,  1.8807e-01,
          3.4062e-01, -3.4014e-01,  1.4475e-02, -6.3424e-03, -2.4901e-01,
         -3.5962e-01, -3.4613e-01],
        [-9.9084e-02,  2.0223e-01, -2.0009e-01,  4.9745e-01, -1.9485e-01,
          4.9564e-01, -2.0212e-01,  8.8657e-03,  1.2250e-01, -2.0077e-01,
         -3.7095e-01, -6.9351e-02,  1.7544e-01,  6.6039e-01, -7.1973e-01,
          2.8015e-01,  1.1970e-01,  2.4213e-01, -2.7106e-01, -6.2333e-01,
          3.3287e-01,  3.9129e-01,  9.6364e-02, -5.8926e-02, -1.4831e-01,
         -3.5544e-01,  6.9212e-02, -5.8652e-01,  6.9777e-01, -3.6264e-01,
         -1.9919e-01, -3.6715e-01],
        [ 1.0853e-01, -4.8432e-01, -3.0028e-02, -5.8753e-01,  8.0826e-03,
         -1.5666e-01,  6.2331e-01, -2.6691e-01, -6.0424e-01,  2.2016e-01,
          6.8287e-01,  6.1698e-01, -1.1399e-02, -6.5303e-01,  3.9492e-01,
         -3.7626e-01,  1.8740e-01, -4.0667e-02, -1.4982e-01,  5.8915e-01,
          3.0004e-01, -3.3637e-01,  3.1619e-01, -1.9725e-01, -4.1655e-01,
          3.9210e-01,  4.4808e-01,  3.2031e-01, -4.5199e-01,  6.9549e-01,
          5.7469e-01,  5.5999e-02],
        [ 4.4422e-01,  4.6489e-01, -3.3630e-01,  7.3455e-01, -6.6456e-01,
          5.6999e-01,  3.2313e-02, -9.8987e-02,  2.6874e-01,  4.3378e-02,
         -1.8236e-01, -6.3266e-01,  3.8009e-01,  4.1941e-01, -8.1641e-02,
         -2.4714e-01, -3.3930e-01, -3.4891e-01,  2.2446e-01,  1.0692e-02,
          2.7154e-01,  6.5136e-01, -4.7841e-02,  3.0273e-01,  1.9552e-01,
         -6.7630e-02,  1.9537e-01, -6.0230e-01,  2.3719e-01, -7.0783e-01,
         -3.8675e-01, -3.1761e-01],
        [-1.7226e-01,  3.7862e-01,  1.8243e-01, -1.6163e-02, -7.4081e-02,
          3.4512e-01,  1.4280e-01, -2.8813e-01,  3.0429e-02, -3.6413e-01,
         -5.8576e-02, -7.9215e-02,  6.8247e-02, -3.6722e-01, -1.1061e-01,
          3.2728e-01,  5.3539e-02,  3.5607e-01,  3.8655e-01, -6.9463e-03,
          1.0259e-01, -3.7125e-01,  1.3520e-01,  3.6911e-01,  4.1771e-01,
         -2.3862e-01,  3.6415e-01, -4.0935e-01,  1.0086e-01, -1.6662e-01,
          2.7962e-01, -4.8024e-01],
        [-4.1424e-01,  8.0298e-02,  8.3991e-02,  5.2828e-01,  5.0196e-03,
          2.7422e-01, -1.0825e-01,  3.4912e-01, -5.1178e-01, -1.6107e-01,
          2.1267e-01, -5.6460e-01,  3.7603e-01, -7.7423e-02, -6.3417e-01,
         -2.0007e-01,  5.8112e-02, -4.2495e-01, -2.2860e-01, -4.3283e-01,
          2.7699e-01, -9.8809e-02, -2.0672e-01, -2.2378e-01,  2.3469e-01,
          1.8091e-01,  3.9570e-01, -1.9950e-01,  4.6967e-01,  1.8430e-01,
          2.8929e-01,  2.5838e-01],
        [-3.0643e-02, -4.8694e-01,  1.2762e-01, -1.5162e-01, -3.3653e-01,
         -3.6942e-01, -1.1300e-01, -1.4796e-01,  1.2252e-01,  3.0381e-02,
          3.0540e-01,  4.1159e-01,  5.1422e-01, -2.6346e-01,  4.1618e-01,
         -3.7622e-01, -9.4461e-02,  4.3903e-01,  3.7091e-01,  5.6811e-02,
          1.0411e-01,  2.7295e-01, -8.5621e-02, -1.0044e-01,  3.0598e-02,
          4.0727e-01,  1.3697e-01,  2.0344e-01, -1.1688e-01,  5.6109e-01,
         -2.7375e-02,  6.9232e-02],
        [ 1.7055e-01,  7.0047e-01,  2.8169e-02,  6.2477e-01, -5.8655e-01,
          6.1288e-01, -3.2207e-01,  3.3199e-01, -2.9648e-01,  7.3484e-02,
         -1.2842e-01, -5.8504e-01, -4.2988e-01,  4.3092e-01, -6.0938e-01,
         -3.5921e-01,  1.3308e-01,  3.0610e-01, -4.6198e-01, -3.8177e-01,
          6.2838e-01,  3.3008e-01, -5.4997e-02, -1.9826e-01,  3.1700e-01,
         -1.8782e-01, -5.3203e-02, -2.6536e-01, -6.1882e-02, -3.9869e-01,
          3.2412e-01, -1.9624e-02],
        [ 2.5623e-01, -3.4197e-01, -4.5112e-01,  4.1539e-01,  2.8567e-01,
         -4.1638e-01, -2.1552e-01,  4.0490e-01,  1.1767e-01,  1.8536e-01,
         -1.1414e-01, -3.0998e-01, -3.5992e-01, -2.3672e-01, -7.3753e-01,
          2.4588e-01, -2.6148e-01, -4.6088e-01, -5.1873e-01, -2.0185e-01,
         -1.9255e-01, -6.2889e-04,  1.2047e-01, -1.7861e-01, -4.8887e-01,
         -7.5521e-02,  4.0825e-01, -2.7466e-01,  3.1663e-01,  3.8428e-02,
          2.6729e-01, -6.3363e-01],
        [-2.4774e-01, -1.2872e-01, -1.6497e-01, -3.3232e-01,  4.4687e-01,
         -6.4880e-01, -1.8911e-01,  5.0240e-04, -6.2014e-01,  4.9852e-01,
         -2.0287e-01,  6.9444e-01,  5.3057e-01, -1.3178e-01,  2.5503e-01,
          2.1466e-01,  5.7413e-01,  3.2561e-01,  6.0006e-01,  5.5113e-01,
         -1.9738e-01, -7.6286e-01, -5.1209e-01, -4.4978e-01, -4.7115e-01,
          1.5884e-01,  3.1530e-01,  6.8362e-01, -2.8085e-01,  3.3323e-01,
          5.0505e-01,  2.1766e-01],
        [-2.0735e-01,  1.3889e-01, -4.5098e-01,  3.3821e-01,  3.1921e-01,
         -2.3040e-01, -1.6901e-01, -1.4435e-01,  3.7574e-01, -2.2332e-01,
         -1.1729e-02,  1.8543e-01, -1.0373e-01,  9.7983e-02, -3.9230e-01,
         -2.4362e-01,  1.0319e-01,  2.6672e-01, -4.0158e-01, -3.7356e-01,
         -2.2983e-01,  5.0387e-01,  4.9800e-01,  1.1858e-01, -3.6304e-01,
          3.4143e-01, -2.8335e-01, -5.0427e-01, -7.6004e-02, -3.2409e-01,
          3.5615e-01,  8.1912e-03],
        [-1.6906e-01, -4.1057e-01,  2.4038e-01, -5.0024e-01,  2.5138e-01,
         -3.7572e-02,  5.9999e-01, -2.2258e-01,  1.3547e-01,  7.6001e-01,
          1.7489e-01,  4.6922e-01,  2.3670e-01, -1.4426e-01, -1.3482e-01,
          1.4852e-02,  3.9478e-01,  5.1564e-01, -1.2506e-01,  3.4714e-02,
         -1.4458e-01, -4.4282e-01, -3.3701e-01, -1.3527e-01, -1.9791e-01,
          1.3480e-02, -9.3338e-02,  4.1162e-01, -7.1528e-01,  2.6702e-01,
          3.1549e-01,  4.5357e-01],
        [-2.6861e-01, -2.6601e-01,  2.8670e-01, -3.9435e-01,  4.0972e-01,
          1.6912e-01, -1.1592e-03, -2.3423e-01,  3.7254e-04,  2.3723e-01,
          1.7876e-01,  4.8743e-01,  4.3629e-01, -2.0199e-01,  5.9381e-01,
          2.7794e-01, -4.3045e-02,  1.0280e-01,  6.8324e-01,  3.1184e-01,
          2.0466e-01, -2.1150e-01,  2.9518e-01,  3.7361e-02, -4.2677e-01,
          5.8798e-01, -3.0294e-01,  6.6400e-01, -6.6461e-01, -2.2182e-01,
          3.8502e-01,  6.0670e-02]])), ('target_linear2.bias', tensor([-0.2301,  0.0401,  0.0596,  0.1120, -0.1803,  0.2044, -0.3782,  0.0218,
         0.0855, -0.0977, -0.3688,  0.0208,  0.1904, -0.0636,  0.1067,  0.1924]))])
>>> test.keys()
odict_keys(['ConvLayer1_AttentionHead0.weight', 'ConvLayer1_AttentionHead0.weight_interact', 'ConvLayer1_AttentionHead0.a', 'ConvLayer1_AttentionHead0.bias', 'ConvLayer1_AttentionHead1.weight', 'ConvLayer1_AttentionHead1.weight_interact', 'ConvLayer1_AttentionHead1.a', 'ConvLayer1_AttentionHead1.bias', 'ConvLayer1_AttentionHead2.weight', 'ConvLayer1_AttentionHead2.weight_interact', 'ConvLayer1_AttentionHead2.a', 'ConvLayer1_AttentionHead2.bias', 'ConvLayer2_AttentionHead0.weight', 'ConvLayer2_AttentionHead0.weight_interact', 'ConvLayer2_AttentionHead0.a', 'ConvLayer2_AttentionHead0.bias', 'ConvLayer2_AttentionHead1.weight', 'ConvLayer2_AttentionHead1.weight_interact', 'ConvLayer2_AttentionHead1.a', 'ConvLayer2_AttentionHead1.bias', 'ConvLayer2_AttentionHead2.weight', 'ConvLayer2_AttentionHead2.weight_interact', 'ConvLayer2_AttentionHead2.a', 'ConvLayer2_AttentionHead2.bias', 'tf_linear1.weight', 'tf_linear1.bias', 'target_linear1.weight', 'target_linear1.bias', 'tf_linear2.weight', 'tf_linear2.bias', 'target_linear2.weight', 'target_linear2.bias'])
>>> from pprint import pprint
>>> pprint(test.keys())
odict_keys(['ConvLayer1_AttentionHead0.weight', 'ConvLayer1_AttentionHead0.weight_interact', 'ConvLayer1_AttentionHead0.a', 'ConvLayer1_AttentionHead0.bias', 'ConvLayer1_AttentionHead1.weight', 'ConvLayer1_AttentionHead1.weight_interact', 'ConvLayer1_AttentionHead1.a', 'ConvLayer1_AttentionHead1.bias', 'ConvLayer1_AttentionHead2.weight', 'ConvLayer1_AttentionHead2.weight_interact', 'ConvLayer1_AttentionHead2.a', 'ConvLayer1_AttentionHead2.bias', 'ConvLayer2_AttentionHead0.weight', 'ConvLayer2_AttentionHead0.weight_interact', 'ConvLayer2_AttentionHead0.a', 'ConvLayer2_AttentionHead0.bias', 'ConvLayer2_AttentionHead1.weight', 'ConvLayer2_AttentionHead1.weight_interact', 'ConvLayer2_AttentionHead1.a', 'ConvLayer2_AttentionHead1.bias', 'ConvLayer2_AttentionHead2.weight', 'ConvLayer2_AttentionHead2.weight_interact', 'ConvLayer2_AttentionHead2.a', 'ConvLayer2_AttentionHead2.bias', 'tf_linear1.weight', 'tf_linear1.bias', 'target_linear1.weight', 'target_linear1.bias', 'tf_linear2.weight', 'tf_linear2.bias', 'target_linear2.weight', 'target_linear2.bias'])
>>> 
>>> pprint(test.keys())
odict_keys(['ConvLayer1_AttentionHead0.weight', 'ConvLayer1_AttentionHead0.weight_interact', 'ConvLayer1_AttentionHead0.a', 'ConvLayer1_AttentionHead0.bias', 'ConvLayer1_AttentionHead1.weight', 'ConvLayer1_AttentionHead1.weight_interact', 'ConvLayer1_AttentionHead1.a', 'ConvLayer1_AttentionHead1.bias', 'ConvLayer1_AttentionHead2.weight', 'ConvLayer1_AttentionHead2.weight_interact', 'ConvLayer1_AttentionHead2.a', 'ConvLayer1_AttentionHead2.bias', 'ConvLayer2_AttentionHead0.weight', 'ConvLayer2_AttentionHead0.weight_interact', 'ConvLayer2_AttentionHead0.a', 'ConvLayer2_AttentionHead0.bias', 'ConvLayer2_AttentionHead1.weight', 'ConvLayer2_AttentionHead1.weight_interact', 'ConvLayer2_AttentionHead1.a', 'ConvLayer2_AttentionHead1.bias', 'ConvLayer2_AttentionHead2.weight', 'ConvLayer2_AttentionHead2.weight_interact', 'ConvLayer2_AttentionHead2.a', 'ConvLayer2_AttentionHead2.bias', 'tf_linear1.weight', 'tf_linear1.bias', 'target_linear1.weight', 'target_linear1.bias', 'tf_linear2.weight', 'tf_linear2.bias', 'target_linear2.weight', 'target_linear2.bias'])
>>> model.eval()
GENELink(
  (ConvLayer1_AttentionHead0): AttentionLayer()
  (ConvLayer1_AttentionHead1): AttentionLayer()
  (ConvLayer1_AttentionHead2): AttentionLayer()
  (ConvLayer2_AttentionHead0): AttentionLayer()
  (ConvLayer2_AttentionHead1): AttentionLayer()
  (ConvLayer2_AttentionHead2): AttentionLayer()
  (tf_linear1): Linear(in_features=64, out_features=32, bias=True)
  (target_linear1): Linear(in_features=64, out_features=32, bias=True)
  (tf_linear2): Linear(in_features=32, out_features=16, bias=True)
  (target_linear2): Linear(in_features=32, out_features=16, bias=True)
)
>>> with open(my_pickle, "rb") as f:
... ccc
  File "<stdin>", line 2
    ccc
    ^
IndentationError: expected an indented block after 'with' statement on line 1
>>> 
>>> with open(my_pickle, "r") as f:
...     data = pickle.load(f)
... 
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
  File "<frozen codecs>", line 322, in decode
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte
>>> with open(my_pickle, "rb") as f:
...     data = pickle.load(f)
... 
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
_pickle.UnpicklingError: A load persistent id instruction was encountered,
but no persistent_load function was specified.
>>> 
>>> model.eval()
GENELink(
  (ConvLayer1_AttentionHead0): AttentionLayer()
  (ConvLayer1_AttentionHead1): AttentionLayer()
  (ConvLayer1_AttentionHead2): AttentionLayer()
  (ConvLayer2_AttentionHead0): AttentionLayer()
  (ConvLayer2_AttentionHead1): AttentionLayer()
  (ConvLayer2_AttentionHead2): AttentionLayer()
  (tf_linear1): Linear(in_features=64, out_features=32, bias=True)
  (target_linear1): Linear(in_features=64, out_features=32, bias=True)
  (tf_linear2): Linear(in_features=32, out_features=16, bias=True)
  (target_linear2): Linear(in_features=32, out_features=16, bias=True)
)
>>> pprint(tf_embed)
tensor([[ 3.7644, -0.0219, -0.0348,  ...,  1.0916, -0.0641, -0.0720],
        [ 3.4793, -0.0170, -0.0287,  ...,  1.1914, -0.0572, -0.0484],
        [ 4.8206, -0.0170, -0.0360,  ...,  2.1023, -0.0742, -0.0822],
        ...,
        [ 4.7907, -0.0168, -0.0356,  ...,  2.0901, -0.0736, -0.0819],
        [ 1.3701,  0.5117,  0.3383,  ...,  0.6054, -0.0169, -0.0204],
        [ 4.8116, -0.0170, -0.0360,  ...,  2.0993, -0.0741, -0.0822]],
       grad_fn=<LeakyReluBackward0>)
>>> adj
tensor(indices=tensor([[  46,  433,   46,  ..., 1077, 1118,  553],
                       [ 433,   46,  536,  ..., 1118,  553, 1118]]),
       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),
       size=(1120, 1120), nnz=39156, layout=torch.sparse_coo)
>>> 


>>> adj
tensor(indices=tensor([[  46,  433,   46,  ..., 1077, 1118,  553],
                       [ 433,   46,  536,  ..., 1118,  553, 1118]]),
       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),
       size=(1120, 1120), nnz=39156, layout=torch.sparse_coo)
>>> tf_embed
tensor([[ 3.7644, -0.0219, -0.0348,  ...,  1.0916, -0.0641, -0.0720],
        [ 3.4793, -0.0170, -0.0287,  ...,  1.1914, -0.0572, -0.0484],
        [ 4.8206, -0.0170, -0.0360,  ...,  2.1023, -0.0742, -0.0822],
        ...,
        [ 4.7907, -0.0168, -0.0356,  ...,  2.0901, -0.0736, -0.0819],
        [ 1.3701,  0.5117,  0.3383,  ...,  0.6054, -0.0169, -0.0204],
        [ 4.8116, -0.0170, -0.0360,  ...,  2.0993, -0.0741, -0.0822]],
       grad_fn=<LeakyReluBackward0>)
>>> 















  [Restored Feb 1, 2024 at 1:34:43AM]
Last login: Thu Feb  1 01:24:13 on console
Restored session: Thu Feb  1 01:13:48 EST 2024
(base) prakritipaul@Prakritis-MacBook-Pro Demo % 
  [Restored Feb 3, 2024 at 8:49:51PM]
Last login: Sat Feb  3 20:49:29 on console
(base) prakritipaul@Prakritis-MacBook-Pro Demo % 
